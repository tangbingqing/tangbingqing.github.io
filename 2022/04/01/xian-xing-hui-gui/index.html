<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.1.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.jpg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.0.0/css/all.min.css" integrity="sha256-jTIdiMuX/e3DGJUGwl3pKSxuc6YOuqtJYkM0bGQESA4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"tangbingqing.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.10.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="一、线性回归 1、线性回归的从零开始实现 2、线性回归的简洁实现   调整学习率 二、softmax回归 1、softmax回归 2、softmax回归的从零开始实现 3、softmax回归的简洁实现   三、多层感知机 1、隐藏层 2、激活函数 3、多层感知机的从零开始实现 4、多层感知机的简介实现   四、模型选择、欠拟合和过拟合 1、训练误差和泛化误差 2、模型选择 3、欠拟合和过拟合 4">
<meta property="og:type" content="article">
<meta property="og:title" content="线性回归">
<meta property="og:url" content="https://tangbingqing.github.io/2022/04/01/xian-xing-hui-gui/index.html">
<meta property="og:site_name" content="T.ng">
<meta property="og:description" content="一、线性回归 1、线性回归的从零开始实现 2、线性回归的简洁实现   调整学习率 二、softmax回归 1、softmax回归 2、softmax回归的从零开始实现 3、softmax回归的简洁实现   三、多层感知机 1、隐藏层 2、激活函数 3、多层感知机的从零开始实现 4、多层感知机的简介实现   四、模型选择、欠拟合和过拟合 1、训练误差和泛化误差 2、模型选择 3、欠拟合和过拟合 4">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tangbingqing.github.io/2022/04/01/xian-xing-hui-gui/AAF60B9F0B2E036CD4773D5513E12FD6.jpg">
<meta property="og:image" content="https://tangbingqing.github.io/2022/04/01/xian-xing-hui-gui/image-20220214144715122.png">
<meta property="og:image" content="https://tangbingqing.github.io/2022/04/01/xian-xing-hui-gui/image-20220302205634394.png">
<meta property="og:image" content="https://tangbingqing.github.io/2022/04/01/xian-xing-hui-gui/image-20220215202316918.png">
<meta property="og:image" content="https://tangbingqing.github.io/2022/04/01/xian-xing-hui-gui/image-20220215202251053.png">
<meta property="og:image" content="https://tangbingqing.github.io/2022/04/01/xian-xing-hui-gui/image-20220215202940748.png">
<meta property="og:image" content="https://tangbingqing.github.io/2022/04/01/xian-xing-hui-gui/image-20220215202956228.png">
<meta property="og:image" content="https://tangbingqing.github.io/2022/04/01/xian-xing-hui-gui/image-20220215203130967.png">
<meta property="og:image" content="https://tangbingqing.github.io/2022/04/01/xian-xing-hui-gui/image-20220305132724165.png">
<meta property="og:image" content="https://tangbingqing.github.io/2022/04/01/xian-xing-hui-gui/image-20220216201329078.png">
<meta property="og:image" content="https://tangbingqing.github.io/2022/04/01/xian-xing-hui-gui/image-20220302203844501.png">
<meta property="og:image" content="https://tangbingqing.github.io/2022/04/01/xian-xing-hui-gui/image-20220302204008100.png">
<meta property="og:image" content="https://tangbingqing.github.io/2022/04/01/xian-xing-hui-gui/image-20220216203059101.png">
<meta property="og:image" content="https://tangbingqing.github.io/2022/04/01/xian-xing-hui-gui/image-20220305132615557.png">
<meta property="og:image" content="https://tangbingqing.github.io/2022/04/01/xian-xing-hui-gui/82C84832C5D58C8BB1BB478C2EF0C1C2.jpg">
<meta property="og:image" content="https://tangbingqing.github.io/2022/04/01/xian-xing-hui-gui/7E030EBDA7519CEAD89CDF91C444EFBE.jpg">
<meta property="article:published_time" content="2022-04-01T08:52:54.000Z">
<meta property="article:modified_time" content="2022-04-01T11:28:26.798Z">
<meta property="article:author" content="T.ng">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tangbingqing.github.io/2022/04/01/xian-xing-hui-gui/AAF60B9F0B2E036CD4773D5513E12FD6.jpg">


<link rel="canonical" href="https://tangbingqing.github.io/2022/04/01/xian-xing-hui-gui/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://tangbingqing.github.io/2022/04/01/xian-xing-hui-gui/","path":"2022/04/01/xian-xing-hui-gui/","title":"线性回归"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>线性回归 | T.ng</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">T.ng</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-主页"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>主页</a></li><li class="menu-item menu-item-标签"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-分类"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-关于"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">一、线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.0.1.</span> <span class="nav-text">1、线性回归的从零开始实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.0.2.</span> <span class="nav-text">2、线性回归的简洁实现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">2.</span> <span class="nav-text">二、softmax回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.0.1.</span> <span class="nav-text">1、softmax回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.0.2.</span> <span class="nav-text">2、softmax回归的从零开始实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.0.3.</span> <span class="nav-text">3、softmax回归的简洁实现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">3.</span> <span class="nav-text">三、多层感知机</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.0.1.</span> <span class="nav-text">1、隐藏层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.0.2.</span> <span class="nav-text">2、激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.0.3.</span> <span class="nav-text">3、多层感知机的从零开始实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.0.4.</span> <span class="nav-text">4、多层感知机的简介实现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">4.</span> <span class="nav-text">四、模型选择、欠拟合和过拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">4.0.1.</span> <span class="nav-text">1、训练误差和泛化误差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">4.0.2.</span> <span class="nav-text">2、模型选择</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">4.0.3.</span> <span class="nav-text">3、欠拟合和过拟合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">4.0.4.</span> <span class="nav-text">4、多项式函数拟合实验</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">5.</span> <span class="nav-text">五、权重衰减(过拟合)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">5.0.1.</span> <span class="nav-text">1、方法（L2范数正则化）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">5.0.2.</span> <span class="nav-text">2、高维线性回归实验</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">6.</span> <span class="nav-text">六、丢弃法（过拟合）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">6.0.1.</span> <span class="nav-text">1、实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">6.0.2.</span> <span class="nav-text">2、简洁实现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">7.</span> <span class="nav-text">七、正向传播、反向传播和计算图</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">8.</span> <span class="nav-text">八、数值稳定性和模型初始化</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">9.</span> <span class="nav-text">九、实战Kaggle比赛：房价预测</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="T.ng"
      src="/images/head.jpg">
  <p class="site-author-name" itemprop="name">T.ng</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">27</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/tangbingqing" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;tangbingqing" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tangbingqing.github.io/2022/04/01/xian-xing-hui-gui/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.jpg">
      <meta itemprop="name" content="T.ng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="T.ng">
      <meta itemprop="description" content="">
    </span>
    
    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="线性回归 | T.ng">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          线性回归
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-04-01 16:52:54 / 修改时间：19:28:26" itemprop="dateCreated datePublished" datetime="2022-04-01T16:52:54+08:00">2022-04-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <ul>
<li><a href="#%E4%B8%80-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92">一、线性回归</a><ul>
<li><a href="#1-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0">1、线性回归的从零开始实现</a></li>
<li><a href="#2-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0">2、线性回归的简洁实现</a></li>
</ul>
</li>
<li><a href="#%E8%B0%83%E6%95%B4%E5%AD%A6%E4%B9%A0%E7%8E%87">调整学习率</a></li>
<li><a href="#%E4%BA%8C-softmax%E5%9B%9E%E5%BD%92">二、softmax回归</a><ul>
<li><a href="#1-softmax%E5%9B%9E%E5%BD%92">1、softmax回归</a></li>
<li><a href="#2-softmax%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0">2、softmax回归的从零开始实现</a></li>
<li><a href="#3-softmax%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0">3、softmax回归的简洁实现</a></li>
</ul>
</li>
<li><a href="#%E4%B8%89-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA">三、多层感知机</a><ul>
<li><a href="#1-%E9%9A%90%E8%97%8F%E5%B1%82">1、隐藏层</a></li>
<li><a href="#2-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">2、激活函数</a></li>
<li><a href="#3-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0">3、多层感知机的从零开始实现</a></li>
<li><a href="#4-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E7%AE%80%E4%BB%8B%E5%AE%9E%E7%8E%B0">4、多层感知机的简介实现</a></li>
</ul>
</li>
<li><a href="#%E5%9B%9B-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9-%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88">四、模型选择、欠拟合和过拟合</a><ul>
<li><a href="#1-%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E5%92%8C%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE">1、训练误差和泛化误差</a></li>
<li><a href="#2-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9">2、模型选择</a></li>
<li><a href="#3-%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88">3、欠拟合和过拟合</a></li>
<li><a href="#4-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%87%BD%E6%95%B0%E6%8B%9F%E5%90%88%E5%AE%9E%E9%AA%8C">4、多项式函数拟合实验</a></li>
</ul>
</li>
<li><a href="#%E4%BA%94-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F%E8%BF%87%E6%8B%9F%E5%90%88">五、权重衰减(过拟合)</a><ul>
<li><a href="#1-%E6%96%B9%E6%B3%95l2%E8%8C%83%E6%95%B0%E6%AD%A3%E5%88%99%E5%8C%96">1、方法（L2范数正则化）</a></li>
<li><a href="#2-%E9%AB%98%E7%BB%B4%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E9%AA%8C">2、高维线性回归实验</a></li>
</ul>
</li>
<li><a href="#%E5%85%AD-%E4%B8%A2%E5%BC%83%E6%B3%95%E8%BF%87%E6%8B%9F%E5%90%88">六、丢弃法（过拟合）</a><ul>
<li><a href="#1-%E5%AE%9E%E7%8E%B0">1、实现</a></li>
<li><a href="#2-%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0">2、简洁实现</a></li>
</ul>
</li>
<li><a href="#%E4%B8%83-%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE">七、正向传播、反向传播和计算图</a></li>
<li><a href="#%E5%85%AB-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96">八、数值稳定性和模型初始化</a></li>
<li><a href="#%E4%B9%9D-%E5%AE%9E%E6%88%98kaggle%E6%AF%94%E8%B5%9B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B">九、实战Kaggle比赛：房价预测</a></li>
</ul>
<!-- tocstop -->

<h1><span id="一-线性回归">一、线性回归</span></h1><h3><span id="1-线性回归的从零开始实现">1、线性回归的从零开始实现</span></h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> matplotlib <span class="token keyword">as</span> plt
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> IPython <span class="token keyword">import</span> display
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> random

<span class="token comment">## 1、生成数据集</span>
<span class="token comment"># y = Xw + b + 噪声</span>
<span class="token keyword">def</span> <span class="token function">synthetic_data</span><span class="token punctuation">(</span>w<span class="token punctuation">,</span>b<span class="token punctuation">,</span>num_examples<span class="token punctuation">)</span><span class="token punctuation">:</span>
    X <span class="token operator">=</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">(</span>num_examples<span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>w<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 均值为0，方差为1 ，随机</span>
    y <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X<span class="token punctuation">,</span>w<span class="token punctuation">)</span> <span class="token operator">+</span> b
    y <span class="token operator">+=</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0.01</span><span class="token punctuation">,</span>y<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
    <span class="token keyword">return</span> X<span class="token punctuation">,</span>y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

true_w <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">3.4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
true_b <span class="token operator">=</span> <span class="token number">4.2</span>
features<span class="token punctuation">,</span>labels <span class="token operator">=</span> synthetic_data<span class="token punctuation">(</span>true_w<span class="token punctuation">,</span>true_b<span class="token punctuation">,</span><span class="token number">1000</span><span class="token punctuation">)</span>

<span class="token comment">## 2、读取数据</span>
<span class="token comment"># 它每次返回 batch_size （批量⼤⼩）个随机样本的特征和标签。</span>
<span class="token keyword">def</span> <span class="token function">data_iter</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span>features<span class="token punctuation">,</span>labels<span class="token punctuation">)</span><span class="token punctuation">:</span>
    num_examples <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>features<span class="token punctuation">)</span>  <span class="token comment">#</span>
    indices <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>num_examples<span class="token punctuation">)</span><span class="token punctuation">)</span>
    random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>indices<span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span>num_examples<span class="token punctuation">,</span>batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        j <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>indices<span class="token punctuation">[</span>i<span class="token punctuation">:</span><span class="token builtin">min</span><span class="token punctuation">(</span>i<span class="token operator">+</span>batch_size<span class="token punctuation">,</span>num_examples<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">yield</span> features<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span>j<span class="token punctuation">)</span><span class="token punctuation">,</span>labels<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span>j<span class="token punctuation">)</span>  <span class="token comment"># 产生小批量的数据</span>

batch_size <span class="token operator">=</span> <span class="token number">10</span>
<span class="token comment"># for X,y in data_iter(batch_size , features , labels):</span>
<span class="token comment">#     print(X,y)</span>

<span class="token comment">## 3、初始化模型参数</span>
w <span class="token operator">=</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0.01</span><span class="token punctuation">,</span>size <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># 需要计算梯度</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment">## 4、定义模型</span>
<span class="token keyword">def</span> <span class="token function">linreg</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span>w<span class="token punctuation">,</span>b<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X<span class="token punctuation">,</span>w<span class="token punctuation">)</span> <span class="token operator">+</span> b

<span class="token comment">## 5、定义损失函数</span>
<span class="token keyword">def</span> <span class="token function">squared_loss</span><span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span>y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment">## 平方损失</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>y_hat <span class="token operator">-</span> y<span class="token punctuation">.</span>view<span class="token punctuation">(</span>y_hat<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">/</span> <span class="token number">2</span>  <span class="token comment"># 返回向量</span>

<span class="token comment">## 6、定义优化算法</span>
<span class="token keyword">def</span> <span class="token function">sgd</span><span class="token punctuation">(</span>params<span class="token punctuation">,</span>lr<span class="token punctuation">,</span>batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">## 参数（w,b），学习率，批量大小</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 更新的时候不要参与梯度计算</span>
        <span class="token keyword">for</span> param <span class="token keyword">in</span> params<span class="token punctuation">:</span>
            param<span class="token punctuation">.</span>data <span class="token operator">-=</span> lr <span class="token operator">*</span> param<span class="token punctuation">.</span>grad <span class="token operator">/</span> batch_size <span class="token comment"># 求了均值</span>
            param<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 梯度设0</span>

<span class="token comment">## 7、训练模型</span>
lr <span class="token operator">=</span> <span class="token number">1e-3</span>
net <span class="token operator">=</span> linreg
loss <span class="token operator">=</span> squared_loss

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> X<span class="token punctuation">,</span>y <span class="token keyword">in</span> data_iter<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span>features<span class="token punctuation">,</span>labels<span class="token punctuation">)</span><span class="token punctuation">:</span>
        l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">,</span>w<span class="token punctuation">,</span>b<span class="token punctuation">)</span><span class="token punctuation">,</span>y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># l是有关⼩批量X和y的损失</span>
        l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># ⼩批量的损失对模型参数求梯度</span>
        sgd<span class="token punctuation">(</span><span class="token punctuation">[</span>w<span class="token punctuation">,</span>b<span class="token punctuation">]</span><span class="token punctuation">,</span>lr<span class="token punctuation">,</span>batch_size<span class="token punctuation">)</span> <span class="token comment"># 使⽤⼩批量随机梯度下降迭代模型参数</span>
        <span class="token comment"># 梯度清零在优化函数中</span>

        <span class="token comment"># 不需要计算梯度的，把整个features放进去，评价一下，计算损失</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        train_l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>features<span class="token punctuation">,</span>w<span class="token punctuation">,</span>b<span class="token punctuation">)</span><span class="token punctuation">,</span>labels<span class="token punctuation">)</span> <span class="token comment"># 小批量的损失</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'epoch %d, loss %f'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> train_l<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3><span id="2-线性回归的简洁实现">2、线性回归的简洁实现</span></h3><p>​        nn 的核⼼数据结构是 ==Module== ，它是⼀个抽象概念，既可以表示神经⽹络中的某个层（layer），也可以表示⼀个包含很多层的神经⽹络。在实际使⽤中，最常⻅的做法是继承 nn.Module ，撰写⾃⼰的⽹络/层。⼀个 nn.Module 实例应该包含⼀些层以及返回输出的前向传播（forward）⽅法。</p>
<blockquote>
<p>​        torch.utils.data 模块提供了有关数据处理的⼯具</p>
<p>​        torch.nn 模块定义了⼤量神经⽹络的层</p>
<p>​        torch.nn.init 模块定义了各种初始化⽅法</p>
<p>​        torch.optim 模块提供了模型参数初始化的各种⽅法。</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> matplotlib <span class="token keyword">as</span> plt
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
form torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Data
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> random
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim

num_inputs <span class="token operator">=</span> <span class="token number">2</span>
num_examples <span class="token operator">=</span> <span class="token number">1000</span>
true_w <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">3.4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
true_b <span class="token operator">=</span> <span class="token number">4.2</span>
batch_size <span class="token operator">=</span> <span class="token number">10</span>


<span class="token comment">## 1、生成数据集</span>
<span class="token comment"># y = Xw + b + 噪声</span>
<span class="token keyword">def</span> <span class="token function">synthetic_data</span><span class="token punctuation">(</span>w<span class="token punctuation">,</span>b<span class="token punctuation">,</span>num_examples<span class="token punctuation">)</span><span class="token punctuation">:</span>
    X <span class="token operator">=</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">(</span>num_examples<span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>w<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 均值为0，方差为1 ，随机</span>
    y <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X<span class="token punctuation">,</span>w<span class="token punctuation">)</span> <span class="token operator">+</span> b
    y <span class="token operator">+=</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0.01</span><span class="token punctuation">,</span>y<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
    <span class="token keyword">return</span> X<span class="token punctuation">,</span>y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

features<span class="token punctuation">,</span>labels <span class="token operator">=</span> synthetic_data<span class="token punctuation">(</span>true_w<span class="token punctuation">,</span>true_b<span class="token punctuation">,</span><span class="token number">1000</span><span class="token punctuation">)</span>

<span class="token comment">## 2、读取数据</span>
<span class="token comment"># PyTorch提供了 data 包来读取数据</span>
<span class="token comment"># 将训练数据的特征和标签组合</span>
dataset <span class="token operator">=</span> data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span>features<span class="token punctuation">,</span>labels<span class="token punctuation">)</span>
<span class="token comment"># 随机读取⼩批量</span>
data_iter <span class="token operator">=</span> data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span>batch_size<span class="token punctuation">,</span>shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>


<span class="token comment">## 3、定义模型</span>
<span class="token triple-quoted-string string">'''
搭建网络的有序容器
net = nn.Sequential(
    nn.Linear(num_inputs, 1)
    # 此处还可以传⼊其他层
	)
'''</span>

<span class="token keyword">class</span> <span class="token class-name">LinearNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>n_feature<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LinearNet<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_feature<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> y

net <span class="token operator">=</span> LinearNet<span class="token punctuation">(</span>num_inputs<span class="token punctuation">)</span>
<span class="token comment"># # 初始化模型参数  # init.normal_ 将权重参数每个元素初始化为随机采样于均值为0、标准差为0.01的正态分布。</span>
<span class="token comment"># net[0].weight.data.normal_(0,0.01)</span>
<span class="token comment"># net[0].bias.data.fill_(0)</span>

<span class="token comment"># 4、损失函数和优化器</span>
loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 均方误差</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr <span class="token operator">=</span> <span class="token number">1e-3</span><span class="token punctuation">)</span>  <span class="token comment">## 传入参数和学习率</span>

<span class="token triple-quoted-string string">'''
# 调整学习率
for param_group in optimizer.param_groups:
 param_group['lr'] *= 0.1 # 学习率为之前的0.1倍
'''</span>

<span class="token comment"># 5、训练模型</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> X<span class="token punctuation">,</span>y <span class="token keyword">in</span> data_iter<span class="token punctuation">:</span>
        output <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span> <span class="token comment"># l是有关⼩批量X和y的损失</span>
        l <span class="token operator">=</span> loss<span class="token punctuation">(</span>output<span class="token punctuation">,</span>y<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 梯度清零</span>
        l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#求了平均</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'epoch %d, loss: %f'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> l<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h1><span id="二-softmax回归">二、softmax回归</span></h1><h3><span id="1-softmax回归">1、softmax回归</span></h3><blockquote>
<p>softmax回归适⽤于分类问题。它使⽤softmax运算输出类别的概率分布。</p>
<p>softmax回归是⼀个单层神经⽹络，输出个数等于分类问题中的类别个数。</p>
</blockquote>
<p><img src="/2022/04/01/xian-xing-hui-gui/AAF60B9F0B2E036CD4773D5513E12FD6.jpg" alt="img"></p>
<p>输出：==最⼤的输出==所对应的类作为预测输出</p>
<p><img src="/2022/04/01/xian-xing-hui-gui/image-20220214144715122.png" alt="image-20220214144715122"></p>
<h3><span id="2-softmax回归的从零开始实现">2、softmax回归的从零开始实现</span></h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torchvision
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pylab <span class="token keyword">as</span> plt
<span class="token keyword">import</span> time
<span class="token keyword">import</span> sys
<span class="token keyword">import</span> d2l



mnist_train <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'~/Datasets/FashionMNIST'</span><span class="token punctuation">,</span>
                                               train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                               download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                               transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
mnist_test <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'~/Datasets/FashionMNIST'</span><span class="token punctuation">,</span>
                                               train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                                               download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                               transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

batch_size <span class="token operator">=</span> <span class="token number">64</span>
train_iter<span class="token punctuation">,</span>test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span>mnist_train<span class="token punctuation">,</span>mnist_test<span class="token punctuation">,</span>batch_size<span class="token punctuation">)</span>  <span class="token comment"># 返回 train_iter 和 test_iter 两个变量</span>

<span class="token comment"># 初始化模型参数</span>
num_inputs <span class="token operator">=</span> <span class="token number">784</span>
num_outputs <span class="token operator">=</span> <span class="token number">10</span>
W <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span>num_outputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">,</span> requires_grad <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_outputs<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">,</span> requires_grad <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># 实现softmax运算</span>
<span class="token comment">## 矩阵 X 的⾏数是样本数，列数是输出个数</span>
<span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 我们将每个元素变成了⾮负数，且每⼀⾏和为1。矩阵每⾏都是合法的概率分布</span>
    <span class="token comment"># softmax运算的输出矩阵中的任意⼀⾏元素代表了⼀个样本在各个输出类别上的预测概率。</span>
    X_exp <span class="token operator">=</span> X<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">)</span>
    partition <span class="token operator">=</span> X_exp<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> X_exp<span class="token operator">/</span>partition
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
X_prob <span class="token operator">=</span> softmax<span class="token punctuation">(</span>X<span class="token punctuation">)</span>

<span class="token comment"># 定义模型</span>
<span class="token keyword">def</span> <span class="token function">net</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> softmax<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>X<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>num_inputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>W<span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token punctuation">)</span> <span class="token comment">##两矩阵相乘</span>

<span class="token comment">##为了得到标签的预测概率，我们可以使⽤ gather 函数</span>
<span class="token keyword">def</span> <span class="token function">cross_entropy</span><span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">##交叉损失函数</span>
    <span class="token keyword">return</span> <span class="token operator">-</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>y_hat<span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 训练模型</span>
num_epochs<span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">0.1</span>
d2l<span class="token punctuation">.</span>train<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> cross_entropy<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> <span class="token punctuation">[</span>W<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token punctuation">)</span>

<span class="token comment"># 预测</span>
X<span class="token punctuation">,</span> y <span class="token operator">=</span> <span class="token builtin">iter</span><span class="token punctuation">(</span>test_iter<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">next</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
true_labels <span class="token operator">=</span> d2l<span class="token punctuation">.</span>get_fashion_mnist_labels<span class="token punctuation">(</span>y<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
pred_labels <span class="token operator">=</span> d2l<span class="token punctuation">.</span>get_fashion_mnist_labels<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
titles <span class="token operator">=</span> <span class="token punctuation">[</span>true <span class="token operator">+</span> <span class="token string">'\n'</span> <span class="token operator">+</span> pred <span class="token keyword">for</span> true<span class="token punctuation">,</span> pred <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>true_labels<span class="token punctuation">,</span>pred_labels<span class="token punctuation">)</span><span class="token punctuation">]</span>
d2l<span class="token punctuation">.</span>show_fashion_mnist<span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">:</span><span class="token number">19</span><span class="token punctuation">]</span><span class="token punctuation">,</span> titles<span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">:</span><span class="token number">19</span><span class="token punctuation">]</span><span class="token punctuation">)</span>




<span class="token comment">###### d2l.py #####</span>
<span class="token comment"># 计算准确率</span>
<span class="token keyword">def</span> <span class="token function">evaluate_accuracy</span><span class="token punctuation">(</span>data_iter<span class="token punctuation">,</span> net<span class="token punctuation">)</span><span class="token punctuation">:</span>
    acc_sum<span class="token punctuation">,</span> n <span class="token operator">=</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0</span>
    <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> data_iter<span class="token punctuation">:</span>
        acc_sum <span class="token operator">+=</span> <span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        n <span class="token operator">+=</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token keyword">return</span> acc_sum <span class="token operator">/</span> n

<span class="token comment"># 优化算法</span>
<span class="token keyword">def</span> <span class="token function">sgd</span><span class="token punctuation">(</span>params<span class="token punctuation">,</span>lr<span class="token punctuation">,</span>batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">## 参数（w,b），学习率，批量大小</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 更新的时候不要参与梯度计算</span>
        <span class="token keyword">for</span> param <span class="token keyword">in</span> params<span class="token punctuation">:</span>
            param<span class="token punctuation">.</span>data <span class="token operator">-=</span> lr <span class="token operator">*</span> param<span class="token punctuation">.</span>grad <span class="token operator">/</span> batch_size <span class="token comment"># 求了均值</span>
            param<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 梯度设0</span>

<span class="token comment"># 训练函数</span>
<span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span>train_iter<span class="token punctuation">,</span>test_iter<span class="token punctuation">,</span>loss<span class="token punctuation">,</span>num_epochs<span class="token punctuation">,</span>batch_size<span class="token punctuation">,</span>params<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>optimizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        train_loss<span class="token punctuation">,</span>train_acc<span class="token punctuation">,</span>n <span class="token operator">=</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0</span>
        <span class="token keyword">for</span> X<span class="token punctuation">,</span>y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>
            y_hat <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span>y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

            <span class="token comment"># 梯度清零</span>
            <span class="token keyword">if</span> optimizer <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">elif</span> params <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> params<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grad <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                <span class="token keyword">for</span> param <span class="token keyword">in</span> params<span class="token punctuation">:</span>
                    param<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>

            l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> optimizer <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                sgd<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

            train_loss <span class="token operator">+=</span> l<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
            train_acc <span class="token operator">+=</span> <span class="token punctuation">(</span>y_hat<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">==</span>y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
            n <span class="token operator">+=</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

        test_acc <span class="token operator">=</span> evaluate_accuracy<span class="token punctuation">(</span>test_iter<span class="token punctuation">,</span> net<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f'</span> <span class="token operator">%</span>
              <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> train_loss <span class="token operator">/</span> n<span class="token punctuation">,</span> train_acc <span class="token operator">/</span> n<span class="token punctuation">,</span>test_acc<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h3><span id="3-softmax回归的简洁实现">3、softmax回归的简洁实现</span></h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
<span class="token keyword">import</span> torchvision
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pylab <span class="token keyword">as</span> plt
<span class="token keyword">import</span> time
<span class="token keyword">import</span> sys
<span class="token keyword">import</span> d2l



mnist_train <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'~/Datasets/FashionMNIST'</span><span class="token punctuation">,</span>
                                               train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                               download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                               transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
mnist_test <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'~/Datasets/FashionMNIST'</span><span class="token punctuation">,</span>
                                               train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                                               download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                               transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

batch_size <span class="token operator">=</span> <span class="token number">64</span>
train_iter<span class="token punctuation">,</span>test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span>mnist_train<span class="token punctuation">,</span>mnist_test<span class="token punctuation">,</span>batch_size<span class="token punctuation">)</span>  <span class="token comment"># 返回 train_iter 和 test_iter 两个变量</span>

<span class="token comment"># 初始化模型参数</span>
num_inputs <span class="token operator">=</span> <span class="token number">784</span>
num_outputs <span class="token operator">=</span> <span class="token number">10</span>

<span class="token comment"># 实现softmax运算</span>
<span class="token keyword">class</span> <span class="token class-name">LinearNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>num_inputs<span class="token punctuation">,</span>num_outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LinearNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span>num_outputs<span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> y

net <span class="token operator">=</span> LinearNet<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span>num_outputs<span class="token punctuation">)</span>
<span class="token comment"># 我们使⽤均值为0、标准差为0.01的正态分布随机初始化模型的权᯿参数。</span>
nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>net<span class="token punctuation">.</span>linear<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>net<span class="token punctuation">.</span>linear<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> val<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>

<span class="token comment"># 训练模型</span>
num_epochs <span class="token operator">=</span> <span class="token number">5</span>
d2l<span class="token punctuation">.</span>train<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> optimizer<span class="token punctuation">)</span>

<span class="token comment"># 预测</span>
X<span class="token punctuation">,</span> y <span class="token operator">=</span> <span class="token builtin">iter</span><span class="token punctuation">(</span>test_iter<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">next</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
true_labels <span class="token operator">=</span> d2l<span class="token punctuation">.</span>get_fashion_mnist_labels<span class="token punctuation">(</span>y<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
pred_labels <span class="token operator">=</span> d2l<span class="token punctuation">.</span>get_fashion_mnist_labels<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
titles <span class="token operator">=</span> <span class="token punctuation">[</span>true <span class="token operator">+</span> <span class="token string">'\n'</span> <span class="token operator">+</span> pred <span class="token keyword">for</span> true<span class="token punctuation">,</span> pred <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>true_labels<span class="token punctuation">,</span>pred_labels<span class="token punctuation">)</span><span class="token punctuation">]</span>
d2l<span class="token punctuation">.</span>show_fashion_mnist<span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">:</span><span class="token number">19</span><span class="token punctuation">]</span><span class="token punctuation">,</span> titles<span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">:</span><span class="token number">19</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>







<h1><span id="三-多层感知机">三、多层感知机</span></h1><p><img src="/2022/04/01/xian-xing-hui-gui/image-20220302205634394.png" alt="image-20220302205634394"></p>
<h3><span id="1-隐藏层">1、隐藏层</span></h3><p>多层感知机在单层神经⽹络的基础上引⼊了⼀到多个隐藏层（hidden layer）。隐藏层位于输⼊层和输出层之间。隐藏层和输出层都是全连接层。</p>
<p><img src="/2022/04/01/xian-xing-hui-gui/image-20220215202316918.png" alt="image-20220215202316918"> <img src="/2022/04/01/xian-xing-hui-gui/image-20220215202251053.png" alt="image-20220215202251053"></p>
<h3><span id="2-激活函数">2、激活函数</span></h3><p>引⼊非线性变换，例如对隐藏变量使⽤按元素运算的非线性函数进⾏变换，然后再作为下⼀个全连接层的输⼊。</p>
<p>1、ReLU函数 : 只保留正数元素，并将负数元素清零</p>
<p><img src="/2022/04/01/xian-xing-hui-gui/image-20220215202940748.png" alt="image-20220215202940748"></p>
<p>2、sigmoid函数 ：可以将元素的值变换到0和1之间</p>
<p><img src="/2022/04/01/xian-xing-hui-gui/image-20220215202956228.png" alt="image-20220215202956228"></p>
<p>3、tanh函数 ：可以将元素的值变换到-1和1之间</p>
<p><img src="/2022/04/01/xian-xing-hui-gui/image-20220215203130967.png" alt="image-20220215203130967"></p>
<h3><span id="3-多层感知机的从零开始实现">3、多层感知机的从零开始实现</span></h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os
<span class="token keyword">from</span> tqdm <span class="token keyword">import</span> tqdm  <span class="token comment">##可扩展的Python进度条，可以在 Python 长循环中添加一个进度提示信息</span>
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transforms
<span class="token keyword">import</span> torchvision
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> tensorboardX <span class="token keyword">import</span> SummaryWriter
<span class="token keyword">import</span> d2l

mnist_train <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'data'</span><span class="token punctuation">,</span>
                                               train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                               download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                               transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
mnist_test <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'data'</span><span class="token punctuation">,</span>
                                               train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                                               download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                               transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

batch_size <span class="token operator">=</span> <span class="token number">128</span>
train_iter<span class="token punctuation">,</span>test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span>mnist_train<span class="token punctuation">,</span>mnist_test<span class="token punctuation">,</span>batch_size<span class="token punctuation">)</span>  <span class="token comment"># 返回 train_iter 和 test_iter 两个变量</span>

<span class="token comment"># 输⼊个数为78,，输出个数为10, 设超参数隐藏单元个数为256</span>

num_inputs<span class="token punctuation">,</span> num_outputs<span class="token punctuation">,</span> num_hiddens <span class="token operator">=</span> <span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">256</span>
w1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>
b1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>

w2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> num_outputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>
b2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_outputs<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>

params <span class="token operator">=</span> <span class="token punctuation">[</span>w1<span class="token punctuation">,</span> b1<span class="token punctuation">,</span> w2<span class="token punctuation">,</span> b2<span class="token punctuation">]</span>
<span class="token keyword">for</span> param <span class="token keyword">in</span> params<span class="token punctuation">:</span>
    param<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># 激活函数</span>
<span class="token keyword">def</span> <span class="token function">relu</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span>X<span class="token punctuation">,</span> other<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">net</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">:</span>
    X <span class="token operator">=</span> X<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> num_inputs<span class="token punctuation">)</span><span class="token punctuation">)</span>
    H <span class="token operator">=</span> relu<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X<span class="token punctuation">,</span>w1<span class="token punctuation">)</span><span class="token operator">+</span>b1<span class="token punctuation">)</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>H<span class="token punctuation">,</span>w2<span class="token punctuation">)</span><span class="token operator">+</span>b2

loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>


num_epochs<span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">100.0</span>  
<span class="token comment">## 学习率这么大：PyTorch默认的是求平均，所以⽤PyTorch计算得到的loss⽐mxnet⼩很多（⼤概是maxnet计算得到的1/batch_size这个量级），所以反向传播得到的梯度也⼩很多</span>
<span class="token comment">## 之所以这么⼤，应该是因为d2l⾥⾯的sgd函数在更新的时候除以了batch_size，其实PyTorch在计算loss的时候已经除过⼀次了，sgd这⾥应该不⽤除了</span>

d2l<span class="token punctuation">.</span>train<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> params<span class="token punctuation">,</span>lr<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h3><span id="4-多层感知机的简介实现">4、多层感知机的简介实现</span></h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os
<span class="token keyword">from</span> tqdm <span class="token keyword">import</span> tqdm  <span class="token comment">##可扩展的Python进度条，可以在 Python 长循环中添加一个进度提示信息</span>
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transforms
<span class="token keyword">import</span> torchvision
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> tensorboardX <span class="token keyword">import</span> SummaryWriter
<span class="token keyword">import</span> d2l

mnist_train <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'data'</span><span class="token punctuation">,</span>
                                               train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                               download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                               transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
mnist_test <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'data'</span><span class="token punctuation">,</span>
                                               train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                                               download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                               transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

batch_size <span class="token operator">=</span> <span class="token number">128</span>
train_iter<span class="token punctuation">,</span>test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span>mnist_train<span class="token punctuation">,</span>mnist_test<span class="token punctuation">,</span>batch_size<span class="token punctuation">)</span>  <span class="token comment"># 返回 train_iter 和 test_iter 两个变量</span>

<span class="token comment"># 输⼊个数为78,，输出个数为10, 设超参数隐藏单元个数为256</span>

num_inputs<span class="token punctuation">,</span> num_outputs<span class="token punctuation">,</span> num_hiddens <span class="token operator">=</span> <span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">256</span>

<span class="token keyword">class</span> <span class="token class-name">LinearNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>num_intputs<span class="token punctuation">,</span> num_outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LinearNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
                nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_intputs<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span><span class="token punctuation">,</span>
                nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> num_outputs<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> y

net <span class="token operator">=</span> LinearNet<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span>num_outputs<span class="token punctuation">)</span>
<span class="token keyword">for</span> params <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>params<span class="token punctuation">,</span>mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>

loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>

num_epochs<span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">0.5</span>

d2l<span class="token punctuation">.</span>train<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span><span class="token boolean">None</span><span class="token punctuation">,</span>optimizer<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><img src="/2022/04/01/xian-xing-hui-gui/image-20220305132724165.png" alt="image-20220305132724165"></p>
<h1><span id="四-模型选择-欠拟合和过拟合">四、模型选择、欠拟合和过拟合</span></h1><h3><span id="1-训练误差和泛化误差">1、训练误差和泛化误差</span></h3><p>训练误差：模型在训练数据集上表现出的误差</p>
<p>泛化误差：模型在任意⼀个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似</p>
<p><strong>机器学习模型应关注降低泛化误差</strong></p>
<h3><span id="2-模型选择">2、模型选择</span></h3><p>1、验证数据集：</p>
<p>2、K折交叉验证：把原始训练数据集分割成K个不重合的子数据集，然后我们做K次模型训练和验证。每⼀次，我们使⽤⼀个⼦数据集验证模型，并使⽤其他K-1个⼦数据集来训练模型。</p>
<h3><span id="3-欠拟合和过拟合">3、欠拟合和过拟合</span></h3><p>欠拟合：是模型⽆法得到较低的训练误差，我们将这⼀现象称作⽋拟合</p>
<p>过拟合：模型的训练误差远⼩于它在测试数据集上的误差，我们称该现象为过拟合</p>
<p>1、模型复杂度：</p>
<p><img src="/2022/04/01/xian-xing-hui-gui/image-20220216201329078.png" alt="image-20220216201329078"></p>
<p>2.训练数据大小：</p>
<p>如果训练数据集中样本数过少，特别是⽐模型参数数量（按元素计）更少时，过拟合更容易发⽣。</p>
<h3><span id="4-多项式函数拟合实验">4、多项式函数拟合实验</span></h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> math

<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
<span class="token keyword">import</span> torchvision
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pylab <span class="token keyword">as</span> plt
<span class="token keyword">import</span> time
<span class="token keyword">import</span> sys
<span class="token keyword">import</span> d2l

<span class="token comment"># 1、生成数据集</span>

max_degree <span class="token operator">=</span> <span class="token number">20</span> <span class="token comment"># 特征为20</span>
n_train<span class="token punctuation">,</span> n_test <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">100</span>  <span class="token comment"># 训练集和测试集</span>
true_w <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>max_degree<span class="token punctuation">)</span>
true_w<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1.2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3.4</span><span class="token punctuation">,</span> <span class="token number">5.6</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># 20个特征，只有前四个是有值的，其余全为噪音</span>
ture_b <span class="token operator">=</span> <span class="token number">5</span>

features <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span>n_train<span class="token operator">+</span>n_test<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>features<span class="token punctuation">)</span>
poly_features <span class="token operator">=</span> np<span class="token punctuation">.</span>power<span class="token punctuation">(</span>features<span class="token punctuation">,</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>max_degree<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_degree<span class="token punctuation">)</span><span class="token punctuation">:</span>
    poly_features<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">/=</span> math<span class="token punctuation">.</span>gamma<span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span>
labels <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>poly_features<span class="token punctuation">,</span>true_w<span class="token punctuation">)</span>
labels <span class="token operator">+=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span>scale<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> size<span class="token operator">=</span>labels<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
true_w<span class="token punctuation">,</span>features<span class="token punctuation">,</span>poly_features<span class="token punctuation">,</span>labels <span class="token operator">=</span> <span class="token punctuation">[</span>
    torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>x<span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
    <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token punctuation">[</span>true_w<span class="token punctuation">,</span>features<span class="token punctuation">,</span>poly_features<span class="token punctuation">,</span>labels<span class="token punctuation">]</span>
<span class="token punctuation">]</span>

<span class="token comment"># print(features[:2], poly_features[:2], labels[:2])</span>


<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>





<h1><span id="五-权重衰减过拟合">五、权重衰减(过拟合)</span></h1><h3><span id="1-方法l2范数正则化">1、方法（L2范数正则化）</span></h3><p>​                           L2范数                                                                       L1范数  </p>
<p><img src="/2022/04/01/xian-xing-hui-gui/image-20220302203844501.png" alt="image-20220302203844501"><img src="/2022/04/01/xian-xing-hui-gui/image-20220302204008100.png" alt="image-20220302204008100"></p>
<p>L2范数正则化在模型==原损失函数==基础上添加 L2范数==惩罚项==，从⽽得到训练所需要最⼩化的函数。</p>
<p><img src="/2022/04/01/xian-xing-hui-gui/image-20220216203059101.png" alt="image-20220216203059101"></p>
<h3><span id="2-高维线性回归实验">2、高维线性回归实验</span></h3><h1><span id="六-丢弃法过拟合">六、丢弃法（过拟合）</span></h1><p>丢弃法：在层之间加入噪音 (只在训练模型时使⽤，当作正则项)</p>
<p>设丢弃概率为p，那么有p的概率hi会被==清零==，有1-p的概率hi会除以1-p做拉伸。丢弃概率是丢弃法的超参数。</p>
<h3><span id="1-实现">1、实现</span></h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os
<span class="token keyword">from</span> tqdm <span class="token keyword">import</span> tqdm  <span class="token comment">##可扩展的Python进度条，可以在 Python 长循环中添加一个进度提示信息</span>
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transforms
<span class="token keyword">import</span> torchvision
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> tensorboardX <span class="token keyword">import</span> SummaryWriter
<span class="token keyword">import</span> d2l

mnist_train <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'data'</span><span class="token punctuation">,</span>
                                               train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                               download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                               transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
mnist_test <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'data'</span><span class="token punctuation">,</span>
                                               train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                                               download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                               transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

batch_size <span class="token operator">=</span> <span class="token number">256</span>
train_iter<span class="token punctuation">,</span>test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span>mnist_train<span class="token punctuation">,</span>mnist_test<span class="token punctuation">,</span>batch_size<span class="token punctuation">)</span>  <span class="token comment"># 返回 train_iter 和 test_iter 两个变量</span>

<span class="token comment"># 输⼊个数为78,，输出个数为10, 设超参数隐藏单元个数为256(两个隐藏层)</span>

num_inputs<span class="token punctuation">,</span> num_outputs<span class="token punctuation">,</span> num_hiddens1<span class="token punctuation">,</span> num_hiddens2 <span class="token operator">=</span> <span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span>
w1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> num_hiddens1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>
b1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_hiddens1<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>

w2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>num_hiddens1<span class="token punctuation">,</span> num_hiddens2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>
b2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_hiddens2<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>

w3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>num_hiddens2<span class="token punctuation">,</span> num_outputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>
b3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_outputs<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>


params <span class="token operator">=</span> <span class="token punctuation">[</span>w1<span class="token punctuation">,</span> b1<span class="token punctuation">,</span> w2<span class="token punctuation">,</span> b2<span class="token punctuation">,</span>w3<span class="token punctuation">,</span>b3<span class="token punctuation">]</span>
<span class="token keyword">for</span> param <span class="token keyword">in</span> params<span class="token punctuation">:</span>
    param<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>


<span class="token comment">#将全连接层和激活函数ReLU串起来，并对每个激活函数的输出使⽤丢弃法</span>
<span class="token comment"># 丢弃函数</span>
<span class="token keyword">def</span> <span class="token function">dropout</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> drop_prob<span class="token punctuation">)</span><span class="token punctuation">:</span>
    X <span class="token operator">=</span> X<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">assert</span> <span class="token number">0</span> <span class="token operator">&lt;=</span> drop_prob <span class="token operator">&lt;=</span> <span class="token number">1</span>
    keep_prob <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> drop_prob
    <span class="token comment"># 这种情况下把全部元素都丢弃</span>
    <span class="token keyword">if</span> keep_prob <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    mask <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">&lt;</span> keep_prob<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> mask <span class="token operator">*</span> X <span class="token operator">/</span> keep_prob
<span class="token comment"># 激活函数</span>
drop_prob1<span class="token punctuation">,</span>drop_prob2 <span class="token operator">=</span> <span class="token number">0.2</span><span class="token punctuation">,</span><span class="token number">0.5</span>
<span class="token keyword">def</span> <span class="token function">relu</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span>X<span class="token punctuation">,</span> other<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">net</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span>is_training<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    X <span class="token operator">=</span> X<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> num_inputs<span class="token punctuation">)</span><span class="token punctuation">)</span>
    H1 <span class="token operator">=</span> relu<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X<span class="token punctuation">,</span>w1<span class="token punctuation">)</span><span class="token operator">+</span>b1<span class="token punctuation">)</span>
    <span class="token keyword">if</span> is_training<span class="token punctuation">:</span>
        H1 <span class="token operator">=</span> dropout<span class="token punctuation">(</span>H1<span class="token punctuation">,</span>drop_prob1<span class="token punctuation">)</span> <span class="token comment">#第一层的丢弃</span>
    H2 <span class="token operator">=</span> relu<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>H1<span class="token punctuation">,</span>w2<span class="token punctuation">)</span><span class="token operator">+</span>b2<span class="token punctuation">)</span>
    <span class="token keyword">if</span> is_training<span class="token punctuation">:</span>
        H2 <span class="token operator">=</span> dropout<span class="token punctuation">(</span>H2<span class="token punctuation">,</span>drop_prob2<span class="token punctuation">)</span> <span class="token comment">#第二层的丢弃</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>H2<span class="token punctuation">,</span>w3<span class="token punctuation">)</span><span class="token operator">+</span>b3


loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>

num_epochs<span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">100.0</span>

d2l<span class="token punctuation">.</span>train<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> params<span class="token punctuation">,</span>lr<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h3><span id="2-简洁实现">2、简洁实现</span></h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os
<span class="token keyword">from</span> tqdm <span class="token keyword">import</span> tqdm  <span class="token comment">##可扩展的Python进度条，可以在 Python 长循环中添加一个进度提示信息</span>
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transforms
<span class="token keyword">import</span> torchvision
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> tensorboardX <span class="token keyword">import</span> SummaryWriter
<span class="token keyword">import</span> d2l

mnist_train <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'data'</span><span class="token punctuation">,</span>
                                               train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                               download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                               transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
mnist_test <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'data'</span><span class="token punctuation">,</span>
                                               train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                                               download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                               transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

batch_size <span class="token operator">=</span> <span class="token number">128</span>
train_iter<span class="token punctuation">,</span>test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span>mnist_train<span class="token punctuation">,</span>mnist_test<span class="token punctuation">,</span>batch_size<span class="token punctuation">)</span>  <span class="token comment"># 返回 train_iter 和 test_iter 两个变量</span>

<span class="token comment"># 输⼊个数为78,，输出个数为10, 设超参数隐藏单元个数为256(两个隐藏层)</span>

num_inputs<span class="token punctuation">,</span> num_outputs<span class="token punctuation">,</span> num_hiddens1<span class="token punctuation">,</span> num_hiddens2 <span class="token operator">=</span> <span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span>
drop_prob1<span class="token punctuation">,</span>drop_prob2 <span class="token operator">=</span> <span class="token number">0.2</span><span class="token punctuation">,</span><span class="token number">0.5</span>

<span class="token keyword">class</span> <span class="token class-name">LinearNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>num_intputs<span class="token punctuation">,</span> num_outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LinearNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_intputs<span class="token punctuation">,</span> num_hiddens1<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>drop_prob1<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens1<span class="token punctuation">,</span>num_hiddens2<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>drop_prob2<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens2<span class="token punctuation">,</span> num_outputs<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> y

net <span class="token operator">=</span> LinearNet<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span>num_outputs<span class="token punctuation">)</span>
<span class="token keyword">for</span> params <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>params<span class="token punctuation">,</span>mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>

loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>

num_epochs<span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">0.5</span>
d2l<span class="token punctuation">.</span>train<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span><span class="token boolean">None</span><span class="token punctuation">,</span>optimizer<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><img src="/2022/04/01/xian-xing-hui-gui/image-20220305132615557.png" alt="image-20220305132615557"></p>
<h1><span id="七-正向传播-反向传播和计算图">七、正向传播、反向传播和计算图</span></h1><p><img src="/2022/04/01/xian-xing-hui-gui/82C84832C5D58C8BB1BB478C2EF0C1C2.jpg" alt="img"></p>
<p><strong>反向累积：</strong></p>
<ol>
<li>构造计算图</li>
<li>前向：执行图，存储中间结果</li>
<li>反向：从相反方向执行图（去除不需要的枝）</li>
</ol>
<p><img src="/2022/04/01/xian-xing-hui-gui/7E030EBDA7519CEAD89CDF91C444EFBE.jpg" alt="img"></p>
<h1><span id="八-数值稳定性和模型初始化">八、数值稳定性和模型初始化</span></h1><h1><span id="九-实战kaggle比赛房价预测">九、实战Kaggle比赛：房价预测</span></h1>
    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/03/30/ji-chu-zhi-shi/" rel="prev" title="基础知识">
                  <i class="fa fa-chevron-left"></i> 基础知识
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/04/01/mo-xing-gou-zao/" rel="next" title="模型构造">
                  模型构造 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">T.ng</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
